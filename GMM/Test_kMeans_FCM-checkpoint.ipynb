{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 40em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506d7c19298547ef9038db77f8ccf4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Start Dataset :  heartDisease\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "kMeans y_train_pred :  [0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
      " 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
      " 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0]\n",
      "\n",
      "\n",
      "Start Dataset :  transfusion\n",
      "[1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0\n",
      " 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "kMeans y_train_pred :  [0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Start Dataset :  wilt\n",
      "[1 1 1 ... 0 0 0]\n",
      "kMeans y_train_pred :  [0 0 0 ... 1 0 0]\n",
      "\n",
      "\n",
      "Finish at  16-06-20---25-03-2021\n"
     ]
    }
   ],
   "source": [
    "# #Display full output\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tqdm.auto import tqdm, trange\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 40em; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import time\n",
    "from pytictoc import TicToc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from fcmeans import FCM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture_mod import GaussianMixtureMod\n",
    "from sklearn.mixture_mod2 import GaussianMixtureMod2\n",
    "from sklearn.mixture_mod3 import GaussianMixtureMod3\n",
    "from sklearn.mixture_mod4 import GaussianMixtureMod4\n",
    "from sklearn.mixture_mod5 import GaussianMixtureMod5\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure,fowlkes_mallows_score,silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score, v_measure_score\n",
    "\n",
    "#Funtion declaration\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "\n",
    "    # Find optimal one-to-one mapping between cluster labels and true labels\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "    print(\"row_ind : \", row_ind)\n",
    "    print(\"col_ind : \", col_ind)\n",
    "    val1 = contingency_matrix[row_ind, col_ind].sum()\n",
    "    print(\"contingency_matrix[row_ind, col_ind].sum() : \", val1)\n",
    "    val2 = np.sum(contingency_matrix)\n",
    "    print(\"np.sum(contingency_matrix) : \", val2)\n",
    "\n",
    "    # Return cluster accuracy\n",
    "    return contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "    \n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "\n",
    "#Get the Clustering Algorithm Performance\n",
    "def cluster_eval(y_true, y_pred, X_train, cluster_label, m, elapsed_time,iter_num):\n",
    "    eval_scores = []\n",
    "    \n",
    "    accuracy = np.mean(y_pred.ravel() == y_true.ravel()) * 100\n",
    "    purity = purity_score(y_train, y_pred)\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1_score = f1_score(y_true, y_pred, average='weighted')\n",
    "    ari_score = adjusted_rand_score(y_true, y_pred)\n",
    "    nmi_score = normalized_mutual_info_score(y_true, y_pred)\n",
    "#     hc_v_measure = homogeneity_completeness_v_measure(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    fm_score = fowlkes_mallows_score(y_true, y_pred)\n",
    "#     s_score = silhouette_score(X_train, y_pred, metric='euclidean')\n",
    "#     db_score = davies_bouldin_score(X_train, y_pred)\n",
    "    eval_scores = [m, cluster_label, elapsed_time, iter_num, accuracy, purity, micro_f1_score, macro_f1_score, weighted_f1_score, ari_score, \n",
    "                   nmi_score, v_measure, fm_score]\n",
    "    return eval_scores\n",
    "    \n",
    "\"\"\"------------------------------------------------------------------------------------------------------------------------\n",
    "#Main Program - Start here!\n",
    "------------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# pd.set_option('display.max_rows', dataset.shape[0]+1)\n",
    "\n",
    "#Define dataset_labels\n",
    "# dataset_labels = [\"balanceScale\"]\n",
    "dataset_labels = [\"heartDisease\",  \"transfusion\", \"wilt\"]  \n",
    "# dataset_labels = [\"transfusion\", \"wilt\", \"breastCancer\", \"heartDisease\", \"eegEyeState\", \"adultIncome\",\n",
    "#                   \"australian\", \"japanese\", \"bank\", \"seismicBumps\", \"german\", \"chess\", \n",
    "#                   \"iris\", \"abalone\", \"wallRobot\", \"dermatology\"]\n",
    "                   \n",
    "# dataset_labels = [\"heartDisease\", \"transfusion\", \"wilt\", \"breastCancer\", \"chess\", \n",
    "#                   \"parkinson\",\"australian\", \"german\", \"japanese\", \"iris\", \"abalone\", \"wallRobot\",\n",
    "#                   \"dermatology\",\"creditApproval\", \"letters\", \"eegEyeState\",\n",
    "#                   \"seismicBumps\",\"bank\", \"adultIncome\", \"sensorlessDrive\"]\n",
    "\n",
    "skip_Mahal = [\"seismicBumps\",\"bank\", \"adultIncome\"]\n",
    "\n",
    "# Threshold of GMM-EM Response values\n",
    "th_resp_val = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# th_resp_val = [1]\n",
    "\n",
    "# resp_type_val = ['donate']\n",
    "# resp_type_val = ['donate']\n",
    "resp_type_val = ['remove', 'donate']\n",
    "\n",
    "result_labels = [\"Dataset\",\"Algorithm\", \"Elapsed Time\", \"Iter_num\",\"accuracy\", \"purity\", \"micro_f1_score\", \"macro_f1_score\", \"weighted_f1_score\", \"ari_score\", \n",
    "                \"nmi_score\", \"hc_v_measure\", \"fm_score\"]\n",
    "\n",
    "all_scores = []\n",
    "final_scores = []\n",
    "all_result = pd.DataFrame()\n",
    "t = TicToc() # create TicToc instance\n",
    "# for m in dataset_labels:\n",
    "for m in tqdm(dataset_labels):\n",
    "#     enablePrint()\n",
    "    print(\"\\n\\nStart Dataset : \",m)\n",
    "    #Read dataset\n",
    "    dataset = pd.read_csv(\"dataset/\"+m+\".csv\")\n",
    "\n",
    "    #Drop Target Column in data using Index\n",
    "    X_train = dataset.drop('Target',axis=1)\n",
    "\n",
    "    # #How to get Target data\n",
    "    y_train =  dataset['Target']\n",
    "\n",
    "    print(np.array(y_train))\n",
    "\n",
    "    n_classes = len(np.unique(y_train))\n",
    "#     cov_type = 'full'\n",
    "    cov_type = 'tied'\n",
    "#     cov_type = 'diag'\n",
    "    #Since we have class labels for the training data, we can initialize the GMM parameters in a supervised manner.\n",
    "    es_means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)])\n",
    "#     print(\"es_means_init :\\n\",es_means_init)\n",
    "\n",
    "    #Run the kMeans on current\n",
    "    kMeans_cluster = KMeans(n_clusters = n_classes, max_iter=20, random_state=0)\n",
    "    t.tic() # Start timer\n",
    "    kMeans_cluster.fit(X_train)\n",
    "    elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    iter_num = kMeans_cluster.n_iter_\n",
    "#     print(\"kMeans time : \", elapsed_time )   \n",
    "    y_train_pred = kMeans_cluster.predict(X_train)\n",
    "    print(\"kMeans y_train_pred : \", y_train_pred )   \n",
    "    eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"kMeans\", m, elapsed_time, iter_num )\n",
    "#     eval_scores = np.append([m],eval_scores,axis=0)  \n",
    "    all_scores = [eval_scores]\n",
    "    \n",
    "#     #Run the Meanshift on current\n",
    "#     ms_cluster = MeanShift(bandwidth=n_classes)\n",
    "#     t.tic() # Start timer\n",
    "#     ms_cluster = ms_cluster.fit(X_train)\n",
    "#     elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#     iter_num = \"NA\" #ms_cluster.n_iter_\n",
    "# #     print(\"GMM time : \", elapsed_time )       \n",
    "#     y_train_pred = ms_cluster.predict(X_train)\n",
    "#     eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"Meanshift\", m, elapsed_time, iter_num )\n",
    "# #     all_scores += [eval_scores]\n",
    "#     all_scores += [eval_scores]\n",
    "    \n",
    "#     #Run the FCM on current\n",
    "#     fcm_cluster = FCM(n_clusters=n_classes)\n",
    "#     t.tic() # Start timer\n",
    "#     fcm_cluster.fit(X_train)\n",
    "#     elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#     iter_num = \"NA\" #fcm_cluster.n_iter_\n",
    "# #     print(\"GMM time : \", elapsed_time )       \n",
    "#     y_train_pred = fcm_cluster.predict(X_train)\n",
    "#     eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"FCM\", m, elapsed_time, iter_num )\n",
    "# #     all_scores += [eval_scores]\n",
    "#     all_scores += [eval_scores]\n",
    "\n",
    "    #Run the GMM on current\n",
    "    gmm_cluster = GaussianMixture(n_components = n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "    t.tic() # Start timer\n",
    "    gmm_cluster.fit(X_train)\n",
    "    elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    iter_num = gmm_cluster.n_iter_\n",
    "#     print(\"GMM time : \", elapsed_time )       \n",
    "    y_train_pred = gmm_cluster.predict(X_train)\n",
    "    eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM\", m, elapsed_time, iter_num )\n",
    "#     all_scores += [eval_scores]\n",
    "    all_scores += [eval_scores]\n",
    "\n",
    "\n",
    "    #Change result to Pandas DataFrame\n",
    "    result = pd.DataFrame(all_scores)\n",
    "    all_result = all_result.append(result)\n",
    "#     print(\"Finish GaussianMixtureMod : \", m, \" : \",resp_type )\n",
    "all_result = all_result.append(pd.Series(), ignore_index=True) \n",
    "all_result.columns=result_labels    \n",
    "\n",
    "\n",
    "#Save results to Excel\n",
    "ts = time.time() \n",
    "st = datetime.datetime.fromtimestamp(ts).strftime('%H-%M-%S---%d-%m-%Y')\n",
    "# all_result.to_excel(\"result/All_result_\"+st+\".xlsx\", index=False)\n",
    "all_result.to_excel(\"result/All_result_\"+cov_type+\"_\"+st+\".xlsx\", index=False)\n",
    "print(\"\\nFinish at \",st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
