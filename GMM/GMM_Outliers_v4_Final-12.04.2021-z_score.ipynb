{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.02.2021 - Modified \"gaussian_mixture_mod5\" \n",
    "- Add new method to get th_resp\n",
    "  - using median of resp\n",
    "  - using center value of min and max of resp ((max - min) /2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Display full output\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tqdm.auto import tqdm, trange\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 40em; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import time\n",
    "from pytictoc import TicToc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from fcmeans import FCM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture_mod import GaussianMixtureMod\n",
    "from sklearn.mixture_mod2 import GaussianMixtureMod2\n",
    "from sklearn.mixture_mod3 import GaussianMixtureMod3\n",
    "from sklearn.mixture_mod4 import GaussianMixtureMod4\n",
    "from sklearn.mixture_mod5 import GaussianMixtureMod5\n",
    "from sklearn.mixture_mod6 import GaussianMixtureMod6\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure,fowlkes_mallows_score,silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score, v_measure_score\n",
    "from scipy.stats import chi2\n",
    "# opening EXCEL through Code local path in dir\n",
    "import os\n",
    "from pathlib import Path\n",
    "#Funtion declaration\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "\n",
    "    # Find optimal one-to-one mapping between cluster labels and true labels\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "    print(\"row_ind : \", row_ind)\n",
    "    print(\"col_ind : \", col_ind)\n",
    "    val1 = contingency_matrix[row_ind, col_ind].sum()\n",
    "    print(\"contingency_matrix[row_ind, col_ind].sum() : \", val1)\n",
    "    val2 = np.sum(contingency_matrix)\n",
    "    print(\"np.sum(contingency_matrix) : \", val2)\n",
    "\n",
    "    # Return cluster accuracy\n",
    "    return contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "    \n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "\n",
    "#Get the Clustering Algorithm Performance\n",
    "def cluster_eval(y_true, y_pred, X_train, cluster_label, m, elapsed_time,iter_num):\n",
    "    eval_scores = []\n",
    "    \n",
    "    accuracy = np.mean(y_pred.ravel() == y_true.ravel()) * 100\n",
    "    \n",
    "    purity = purity_score(y_train, y_pred)\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1_score = f1_score(y_true, y_pred, average='weighted')\n",
    "    ari_score = adjusted_rand_score(y_true, y_pred)\n",
    "    nmi_score = normalized_mutual_info_score(y_true, y_pred)\n",
    "#     hc_v_measure = homogeneity_completeness_v_measure(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    fm_score = fowlkes_mallows_score(y_true, y_pred)\n",
    "#     s_score = silhouette_score(X_train, y_pred, metric='euclidean')\n",
    "#     db_score = davies_bouldin_score(X_train, y_pred)\n",
    "    eval_scores = [m, cluster_label, elapsed_time, iter_num, accuracy, purity, micro_f1_score, macro_f1_score, weighted_f1_score, ari_score, \n",
    "                   nmi_score, v_measure, fm_score]\n",
    "    return eval_scores\n",
    "\n",
    "#function to remove outliers using z-score\n",
    "def z_score(data):\n",
    "    data_z = data['mean']\n",
    "    outliers_z = data_z.copy()\n",
    "    mean_z = np.mean(data_z)\n",
    "    std_z =np.std(data_z)\n",
    "    threshold=3.\n",
    "\n",
    "    data['z_score'] = (outliers_z - mean_z)/std_z \n",
    "    outliers_z[(data['z_score'] > threshold)] = np.nan\n",
    "    print('outliers_z : \\n', outliers_z)\n",
    "    #Drop outliers\n",
    "    no_outliers_z = outliers_z.dropna()\n",
    "    print('outliers_z_new : \\n', no_outliers_z)    \n",
    "    return no_outliers_z\n",
    "\n",
    "#function to calculate means \n",
    "def means_init(X, n_components):\n",
    "    n_samples, n_attributes = X.shape\n",
    "    print(\"n_components : \", n_components)\n",
    "    \n",
    "    df_X = pd.DataFrame(X)\n",
    "    \n",
    "    df_X['mean'] = df_X.mean(axis=1)\n",
    "    \n",
    "    no_outliers_z = z_score(df_X)\n",
    "\n",
    "    #Find Max, Min, Diff, Dev\n",
    "    Max = max(no_outliers_z)\n",
    "    Min = min(no_outliers_z)\n",
    "    \n",
    "    diff = Max - Min\n",
    "    dev = diff/n_components\n",
    "    \n",
    "    print('Max: ', Max , 'Min: ', Min)   \n",
    "    print('dev :', dev)\n",
    "    \n",
    "    cluster_range = np.zeros((n_components, 2))\n",
    "    for i in range (n_components):\n",
    "        cluster_range[i] = Min, Min+dev\n",
    "        \n",
    "        Min = Min+dev\n",
    "        df_X['c'+str(i)]=0\n",
    "    print(cluster_range)  \n",
    "\n",
    "    for j in range (n_components):\n",
    "        df_X['c'+str(j)][(df_X['mean']>= cluster_range[j,0]) & (df_X['mean'] < cluster_range[j,1])] = 1\n",
    "\n",
    "    es_init_means = np.zeros((n_components, n_attributes))\n",
    "    for k in range (n_components):    \n",
    "        temp_df_X = df_X.iloc[:, 0:n_attributes][df_X['c'+str(k)] == 1]   \n",
    "        es_init_means[k] = temp_df_X.mean(axis=0) \n",
    "    return es_init_means\n",
    "\"\"\"------------------------------------------------------------------------------------------------------------------------\n",
    "#Main Program - Start here!\n",
    "------------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# pd.set_option('display.max_rows', dataset.shape[0]+1)\n",
    "\n",
    "#Define dataset_labels\n",
    "# dataset_labels = [ \"haberman\", \"landsat\", \"iris\", \"seed\" ]\n",
    "# dataset_labels = [\"ecoli\"]\"glass\",\"vertebral\",\"new_thyroid\",\n",
    "# dataset_labels = [\"wine\", \"new_thyroid\"]\n",
    "# dataset_labels = [\"transfusion\", \"wilt\", \"australian\", \"japanese\", \"iris\"]\n",
    "# dataset_labels = [\"transfusion\", \"wilt\", \"breastCancer\", \"heartDisease\", \"adultIncome\", \"australian\", \"japanese\", \"bank\", \"seismicBumps\", \"german\", \n",
    "#                   \"chess\", \"iris\", \"abalone\", \"wallRobot\", \"dermatology\"] \n",
    "dataset_labels = [\"iris\"]            \n",
    "# dataset_labels = [\"ecoli\", \"seed\", \"glass\"]\n",
    "\n",
    "# skip_Mahal = [\"australian\",\"japanese\", \"seismicBumps\",\"bank\", \"adultIncome\"]\n",
    "\n",
    "# cov_type_labels = [\"full\", \"tied\", \"diag\", \"spherical\"]\n",
    "cov_type_labels = [\"full\"]\n",
    "# cov_type_labels = [\"diag\", \"spherical\"]\n",
    "# Threshold of GMM-EM Response values\n",
    "# th_resp_val = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# th_resp_val = [1]\n",
    "\n",
    "# resp_type_val = ['remove']\n",
    "# resp_type_val = ['donate']\n",
    "resp_type_val = ['remove', 'donate']\n",
    "\n",
    "result_labels = [\"Dataset\",\"Algorithm\", \"Elapsed Time\", \"Iter_num\",\"accuracy\", \"purity\", \"micro_f1_score\", \"macro_f1_score\", \"weighted_f1_score\", \"ari_score\", \n",
    "                \"nmi_score\", \"hc_v_measure\", \"fm_score\"]\n",
    "absolutePath = []\n",
    "for cov_type in (cov_type_labels):\n",
    "    print(\"\\nStart Cov_Type :\",cov_type)\n",
    "                   \n",
    "    all_scores = []\n",
    "    final_scores = []\n",
    "    all_result = pd.DataFrame()\n",
    "    t = TicToc() # create TicToc instance\n",
    "    # for m in dataset_labels:\n",
    "    for m in tqdm(dataset_labels):\n",
    "    #     enablePrint()\n",
    "        print(\"\\n\\nStart Dataset : \",m)\n",
    "        #Read dataset\n",
    "        dataset = pd.read_csv(\"dataset/\"+m+\".csv\")\n",
    "\n",
    "        #Drop Target Column in data using Index\n",
    "        X_train = dataset.drop('Target',axis=1)\n",
    "\n",
    "        # #How to get Target data\n",
    "        y_train =  dataset['Target']\n",
    "\n",
    "        print(np.array(y_train))\n",
    "\n",
    "        n_classes = len(np.unique(y_train))\n",
    "\n",
    "        #     cov_type = 'full'\n",
    "    #     cov_type = 'full'\n",
    "    #     cov_type = 'diag'\n",
    "#         Since we have class labels for the training data, we can initialize the GMM parameters in a supervised manner.\n",
    "        es_means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)])\n",
    "\n",
    "    #     es_means_init = np.array ([[5.005555556, 3.344444444, 1.596296296, 0.303703704],\n",
    "    #                                 [5.973333333, 2.775, 4.503333333, 1.468333333],\n",
    "#     #                                 [6.86, 3.077142857, 5.731428571, 2.091428571]]) \n",
    "\n",
    "        es_means_init2_method = np.array ([[5.006, 3.428, 1.462, 0.246],\n",
    "                                          [5.95178571, 2.77678571, 4.3375, 1.375],\n",
    "                                          [6.65681818, 2.99318182, 5.62954545, 2.05909091]])\n",
    "    \n",
    "        means__init = np.array ([ [5.00555556, 3.34444444, 1.5962963,  0.3037037 ],\n",
    "                                    [5.97333333, 2.775,      4.50333333, 1.46833333],\n",
    "                                    [6.88333333, 3.09722222, 5.75833333, 2.09444444] ])   \n",
    "\n",
    "        medians_init = np.array ([[5.,   3.4,  1.5,  0.2 ],\n",
    "                                     [6.,   2.8,  4.5,  1.4 ],\n",
    "                                     [6.75, 3.05, 5.7,  2.1 ]])  \n",
    "        \n",
    "        means_init_hybrid = np.array ([ [5.006,      3.428,      1.462,      0.246     ],\n",
    "                                         [5.95172414, 2.77241379, 4.35862069, 1.39482759],\n",
    "                                         [6.69047619, 3.00952381, 5.66190476, 2.06428571] ])   \n",
    "\n",
    "        medians_init_hybrid = np.array ([[5.,   3.4,  1.5,  0.2 ],\n",
    "                                         [6.,   2.8,  4.45, 1.4 ],\n",
    "                                         [6.7,  3.,   5.6,  2.1 ]])\n",
    "        \n",
    "        means_init_attr = np.array ([[4.95384615, 3.26981132, 3.203,      0.99      ],\n",
    "                                     [6.00857143, 2.87971014, 4.74680851, 1.58297872],\n",
    "                                     [7.05185185, 3.06666667, 6.76666667, 2.16666667]])\n",
    "        \n",
    "         \n",
    "#         X = X_train.copy()\n",
    "#         es_means_init = means_init(X, n_classes)\n",
    "#         print(\"es_means_init :\\n\",es_means_init)\n",
    "#         if (m == \"wilt\"):\n",
    "#     #         es_means_init[[0, 1]] = es_means_init[[1, 0]] \n",
    "#             print(\"es_means_init_\"+m+\" :\\n\",es_means_init) \n",
    "#         elif (m == \"heartDisease\"):\n",
    "#             es_means_init[[0, 1]] = es_means_init[[1, 0]] \n",
    "#             print(\"es_means_init_\"+m+\" :\\n\",es_means_init) \n",
    "#         elif (m == \"adultIncome\"):\n",
    "#             es_means_init[[0, 1]] = es_means_init[[1, 0]] \n",
    "#             print(\"es_means_init_\"+m+\" :\\n\",es_means_init) \n",
    "#         elif (m == \"australian\"):\n",
    "#             es_means_init[[0, 1]] = es_means_init[[1, 0]] \n",
    "#             print(\"es_means_init_\"+m+\" :\\n\",es_means_init)   \n",
    "#         elif (m == \"wallRobot\"):\n",
    "#             es_means_init[[0, 1, 2, 3]] = es_means_init[[1, 2, 3, 0]] \n",
    "#             print(\"es_means_init_\"+m+\" :\\n\",es_means_init)     \n",
    "#         elif (m == \"dermatology\"):\n",
    "#             es_means_init[[0, 1, 2, 3, 4, 5]] = es_means_init[[3, 5, 4, 1, 0, 2]] \n",
    "#             print(\"es_means_init_\"+m+\" :\\n\",es_means_init) \n",
    "#     #     es_means_init = None\n",
    "\n",
    "        #Run the kMeans on current\n",
    "        kMeans_cluster = KMeans(n_clusters = n_classes, max_iter=20, random_state=0)\n",
    "        t.tic() # Start timer\n",
    "        kMeans_cluster.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = kMeans_cluster.n_iter_\n",
    "    #     print(\"kMeans time : \", elapsed_time )   \n",
    "        y_train_pred = kMeans_cluster.predict(X_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"kMeans\", m, elapsed_time, iter_num )\n",
    "    #     eval_scores = np.append([m],eval_scores,axis=0)  \n",
    "        all_scores = [eval_scores]\n",
    "\n",
    "    #     #Run the Meanshift on current\n",
    "    #     ms_cluster = MeanShift(bandwidth=n_classes)\n",
    "    #     t.tic() # Start timer\n",
    "    #     ms_cluster = ms_cluster.fit(X_train)\n",
    "    #     elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    #     iter_num = \"NA\" #ms_cluster.n_iter_\n",
    "    # #     print(\"GMM time : \", elapsed_time )       \n",
    "    #     y_train_pred = ms_cluster.predict(X_train)\n",
    "    #     eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"Meanshift\", m, elapsed_time, iter_num )\n",
    "    # #     all_scores += [eval_scores]\n",
    "    #     all_scores += [eval_scores]\n",
    "\n",
    "    #     #Run the FCM on current\n",
    "    #     fcm_cluster = FCM(n_clusters=n_classes)\n",
    "    #     t.tic() # Start timer\n",
    "    #     fcm_cluster.fit(X_train)\n",
    "    #     elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    #     iter_num = \"NA\" #fcm_cluster.n_iter_\n",
    "    # #     print(\"GMM time : \", elapsed_time )       \n",
    "    #     y_train_pred = fcm_cluster.predict(X_train)\n",
    "    #     eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"FCM\", m, elapsed_time, iter_num )\n",
    "    # #     all_scores += [eval_scores]\n",
    "    #     all_scores += [eval_scores]\n",
    "        \n",
    "        #Run the GMM on current\n",
    "        gmm_cluster = GaussianMixture(n_components = n_classes, init_params='random', means_init= None, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster.n_iter_\n",
    "    #     print(\"GMM time : \", elapsed_time )       \n",
    "        y_train_pred = gmm_cluster.predict(X_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM-random\", m, elapsed_time, iter_num )\n",
    "    #     all_scores += [eval_scores]\n",
    "        all_scores += [eval_scores]\n",
    "        \n",
    "                #Run the GMM on current\n",
    "        gmm_cluster = GaussianMixture(n_components = n_classes, init_params='kmeans', means_init= None, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster.n_iter_\n",
    "    #     print(\"GMM time : \", elapsed_time )       \n",
    "        y_train_pred = gmm_cluster.predict(X_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM-kmeans\", m, elapsed_time, iter_num )\n",
    "    #     all_scores += [eval_scores]\n",
    "        all_scores += [eval_scores]\n",
    "        \n",
    "#                  #Run the GMM on current\n",
    "#         gmm_cluster = GaussianMixture(n_components = n_classes, means_init= means_init_attr, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "#         t.tic() # Start timer\n",
    "#         gmm_cluster.fit(X_train)\n",
    "#         elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#         iter_num = gmm_cluster.n_iter_\n",
    "#     #     print(\"GMM time : \", elapsed_time )       \n",
    "#         y_train_pred = gmm_cluster.predict(X_train)\n",
    "#         eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM-means_init_attr\", m, elapsed_time, iter_num )\n",
    "#     #     all_scores += [eval_scores]\n",
    "#         all_scores += [eval_scores]\n",
    "        \n",
    "        #Run the GMM on current\n",
    "        gmm_cluster = GaussianMixture(n_components = n_classes, means_init= es_means_init, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster.n_iter_\n",
    "    #     print(\"GMM time : \", elapsed_time )       \n",
    "        y_train_pred = gmm_cluster.predict(X_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM-es_means_init\", m, elapsed_time, iter_num )\n",
    "    #     all_scores += [eval_scores]\n",
    "        all_scores += [eval_scores]\n",
    "        \n",
    "        \n",
    "         #Run the GMM on current\n",
    "        gmm_cluster = GaussianMixture(n_components = n_classes, init_params='jaha_init1', means_init= None, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster.n_iter_\n",
    "    #     print(\"GMM time : \", elapsed_time )       \n",
    "        y_train_pred = gmm_cluster.predict(X_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM-init1\", m, elapsed_time, iter_num )\n",
    "    #     all_scores += [eval_scores]\n",
    "        all_scores += [eval_scores]\n",
    "        \n",
    "                 #Run the GMM on current\n",
    "        gmm_cluster = GaussianMixture(n_components = n_classes, init_params='jaha_init_hybrid', means_init= None, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster.n_iter_\n",
    "    #     print(\"GMM time : \", elapsed_time )       \n",
    "        y_train_pred = gmm_cluster.predict(X_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM-init_hybrid\", m, elapsed_time, iter_num )\n",
    "    #     all_scores += [eval_scores]\n",
    "        all_scores += [eval_scores]\n",
    "\n",
    "    #     #Run the GMM Mahalanobis on current - Last tested on 28.01.2021\n",
    "    #     if m not in (skip_Mahal):    \n",
    "    #         for resp_type in resp_type_val:\n",
    "    #             gmm_cluster_mod2 = GaussianMixtureMod2(n_components = n_classes, init_params='kmeans', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type)\n",
    "    #             t.tic() # Start timer\n",
    "    #             gmm_cluster_mod2.fit(X_train)\n",
    "    #             elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    #             iter_num = gmm_cluster_mod2.n_iter_\n",
    "    #         #     print(\"GMM time : \", elapsed_time ) \n",
    "    #             resp_labels = \"Mahalanobis-\"+str(resp_type)\n",
    "    #             y_train_pred_mod2 = gmm_cluster_mod2.predict(X_train)\n",
    "    #             eval_scores = cluster_eval(y_train, y_train_pred_mod2, X_train, resp_labels, m, elapsed_time, iter_num )\n",
    "    #             all_scores += [eval_scores]\n",
    "    #         print(\"Finish GaussianMixtureMod2/Mahalanobis : \",m,\" : \",resp_type )\n",
    "    #     else:\n",
    "    #         print(\"Skip GaussianMixtureMod2/Mahalanobis for :\",m )\n",
    "\n",
    "        # Run the GMM-Auto for every th_resp value on current dataset (using intersect)- Last tested on 27.01.2021\n",
    "        #     for resp_type in resp_type_val:\n",
    "        #         gmm_cluster_mod3 = GaussianMixtureMod3(n_components=n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "        #         t.tic() # Start timer\n",
    "        #         gmm_cluster_mod3.fit(X_train)\n",
    "        #         elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        #         iter_num = gmm_cluster_mod3.n_iter_     \n",
    "        #         resp_labels = \"GMM_Auto-\"+str(resp_type)\n",
    "        #         y_train_pred_mod3 = gmm_cluster_mod3.predict(X_train)\n",
    "        #         eval_scores = cluster_eval(y_train, y_train_pred_mod3, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "        #         all_scores += [eval_scores]\n",
    "        #     print(\"Finish GaussianMixtureMod3 : \", m,\" : \",resp_type )\n",
    "        \n",
    "          #Run the GMM-Auto-v2 for every th_resp value on current dataset (using weights*cv or cv/2) - Last tested on 27.01.2021\n",
    " \n",
    "#comment by Azha on 9.9.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod4 = GaussianMixtureMod4(n_components=n_classes, init_params='random', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod4.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod4.n_iter_     \n",
    "            resp_labels = \"resp Mixing Coefficients-random_\"+str(resp_type)\n",
    "            y_train_pred_mod4v = gmm_cluster_mod4.predict(X_train)\n",
    "#             y_train_pred_mod4v = np.choose(y_train_pred_mod4v,(4,5,0,1,2,3)).astype(np.int64) \n",
    "            print('y_train_pred_mod4v :\\n', y_train_pred_mod4v)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod4v, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod4/resp Mixing Coefficients random  : \", m, \" : \",resp_type )\n",
    "        \n",
    "          #Run the GMM-Auto-v2 for every th_resp value on current dataset (using weights*cv or cv/2) - Last tested on 27.01.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod4 = GaussianMixtureMod4(n_components=n_classes, init_params='kmeans', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod4.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod4.n_iter_     \n",
    "            resp_labels = \"resp Mixing Coefficients-kmeans_\"+str(resp_type)\n",
    "            y_train_pred_mod4v = gmm_cluster_mod4.predict(X_train)\n",
    "#             y_train_pred_mod4v = np.choose(y_train_pred_mod4v,(4,5,0,1,2,3)).astype(np.int64) \n",
    "            print('y_train_pred_mod4v :\\n', y_train_pred_mod4v)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod4v, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod4/resp Mixing Coefficients kmeans  : \", m, \" : \",resp_type )\n",
    "        \n",
    "        #Run the GMM-Auto-v2 for every th_resp value on current dataset (using weights*cv or cv/2) - Last tested on 27.01.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod4 = GaussianMixtureMod4(n_components=n_classes, init_params='jaha_init1', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod4.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod4.n_iter_     \n",
    "            resp_labels = \"resp Mixing Coefficients-init1_\"+str(resp_type)\n",
    "            y_train_pred_mod4v = gmm_cluster_mod4.predict(X_train)\n",
    "#             y_train_pred_mod4v = np.choose(y_train_pred_mod4v,(4,5,0,1,2,3)).astype(np.int64) \n",
    "            print('y_train_pred_mod4v :\\n', y_train_pred_mod4v)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod4v, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod4/resp Mixing Coefficients init1  : \", m, \" : \",resp_type )\n",
    "        \n",
    "        #Run the GMM-Auto-v2 for every th_resp value on current dataset (using weights*cv or cv/2) - Last tested on 27.01.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod4 = GaussianMixtureMod4(n_components=n_classes, init_params='jaha_init_hybrid', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod4.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod4.n_iter_     \n",
    "            resp_labels = \"resp Mixing Coefficients-init_hybrid_\"+str(resp_type)\n",
    "            y_train_pred_mod4 = gmm_cluster_mod4.predict(X_train)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod4, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod4/resp Mixing Coefficients-init_hybrid  : \", m, \" : \",resp_type )\n",
    "               \n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod5 = GaussianMixtureMod5(n_components=n_classes, init_params='random', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod5.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod5.n_iter_     \n",
    "            resp_labels = \"resp center values-random\"+str(resp_type)\n",
    "            y_train_pred_mod5 = gmm_cluster_mod5.predict(X_train)\n",
    "#             y_train_pred_mod5 = np.choose(y_train_pred_mod5,(4,5,0,1,2,3)).astype(np.int64) \n",
    "#             y_train_pred_mod5 = np.flip(y_train_pred_mod5)#5,1,0,3,2,5\n",
    "            print('y_train_pred_mod5 :\\n', y_train_pred_mod5)\n",
    "            print('y_train :\\n', y_train)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod5, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod5/resp center values-random : \", m, \" : \",resp_type )\n",
    "        \n",
    "        #Run the GMM-Auto-v2 for every th_resp value on current dataset (using resp center values) - Last tested on 27.01.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod5 = GaussianMixtureMod5(n_components=n_classes, init_params='kmeans', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod5.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod5.n_iter_     \n",
    "            resp_labels = \"resp center values-kmeans\"+str(resp_type)\n",
    "            y_train_pred_mod5 = gmm_cluster_mod5.predict(X_train)\n",
    "#             y_train_pred_mod5 = np.choose(y_train_pred_mod5,(4,5,0,1,2,3)).astype(np.int64) \n",
    "#             y_train_pred_mod5 = np.flip(y_train_pred_mod5)#5,1,0,3,2,5\n",
    "            print('y_train_pred_mod5 :\\n', y_train_pred_mod5)\n",
    "            print('y_train :\\n', y_train)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod5, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod5/resp center values-kmeans : \", m, \" : \",resp_type)   \n",
    "        \n",
    "#                 #Run the GMM-Auto-v2 for every th_resp value on current dataset (using resp center values) - Last tested on 27.01.2021\n",
    "#         for resp_type in resp_type_val:\n",
    "#             gmm_cluster_mod5 = GaussianMixtureMod5(n_components=n_classes, means_init=means_init_attr, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "#             t.tic() # Start timer\n",
    "#             gmm_cluster_mod5.fit(X_train)\n",
    "#             elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#             iter_num = gmm_cluster_mod5.n_iter_     \n",
    "#             resp_labels = \"resp center values-means_init_attr\"+str(resp_type)\n",
    "#             y_train_pred_mod5 = gmm_cluster_mod5.predict(X_train)\n",
    "# #             y_train_pred_mod5 = np.choose(y_train_pred_mod5,(4,5,0,1,2,3)).astype(np.int64) \n",
    "# #             y_train_pred_mod5 = np.flip(y_train_pred_mod5)#5,1,0,3,2,5\n",
    "#             print('y_train_pred_mod5 :\\n', y_train_pred_mod5)\n",
    "#             print('y_train :\\n', y_train)\n",
    "#             eval_scores = cluster_eval(y_train, y_train_pred_mod5, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "#             all_scores += [eval_scores]\n",
    "#         print(\"Finish GaussianMixtureMod5/resp center values-means_init_attr : \", m, \" : \",resp_type)   \n",
    "        \n",
    "         #Run the GMM-Auto-v2 for every th_resp value on current dataset (using resp center values) - Last tested on 27.01.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod5 = GaussianMixtureMod5(n_components=n_classes, init_params='jaha_init1', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod5.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod5.n_iter_     \n",
    "            resp_labels = \"resp center values-init1_\"+str(resp_type)\n",
    "            y_train_pred_mod5 = gmm_cluster_mod5.predict(X_train)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod5, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod5/resp center values init1 : \", m, \" : \",resp_type )\n",
    "\n",
    "        #Run the GMM-Auto-v2 for every th_resp value on current dataset (using resp center values) - Last tested on 27.01.2021\n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod5 = GaussianMixtureMod5(n_components=n_classes, init_params='jaha_init_hybrid', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod5.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod5.n_iter_     \n",
    "            resp_labels = \"resp center values-init_hybrid_\"+str(resp_type)\n",
    "            y_train_pred_mod5 = gmm_cluster_mod5.predict(X_train)\n",
    "#             y_train_pred_mod5 = np.choose(y_train_pred_mod5,(4,5,0,1,2,3)).astype(np.int64) \n",
    "#             y_train_pred_mod5 = np.flip(y_train_pred_mod5)#5,1,0,3,2,5\n",
    "            print('y_train_pred_mod5 :\\n', y_train_pred_mod5)\n",
    "            print('y_train :\\n', y_train)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod5, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod5/resp center values init_hybrid: \", m, \" : \",resp_type )\n",
    "\n",
    "#         #Run the \n",
    "#         gmm_cluster_mod6 = GaussianMixtureMod6(n_components=n_classes, means_init=means_init_attr, covariance_type=cov_type, max_iter=20, random_state=0 )\n",
    "#         t.tic() # Start timer\n",
    "#         gmm_cluster_mod6.fit(X_train)\n",
    "#         elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#         iter_num = gmm_cluster_mod6.n_iter_     \n",
    "#         resp_labels = \"outliers based-init_attr_\"\n",
    "#         y_train_pred_mod6 = gmm_cluster_mod6.predict(X_train)\n",
    "        \n",
    "#         print('y_train_pred_mod6 :\\n', y_train_pred_mod6)\n",
    "#         print('y_train :\\n', y_train)\n",
    "#         eval_scores = cluster_eval(y_train, y_train_pred_mod6, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "#         all_scores += [eval_scores]\n",
    "#         print(\"Finish GaussianMixtureMod6/outliers based-means_init_attr: \", m )\n",
    "        \n",
    "         #Run the \n",
    "        gmm_cluster_mod6 = GaussianMixtureMod6(n_components=n_classes, means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0 )\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster_mod6.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster_mod6.n_iter_     \n",
    "        resp_labels = \"outliers based-es_means_init\"\n",
    "        y_train_pred_mod6 = gmm_cluster_mod6.predict(X_train)\n",
    "\n",
    "        print('y_train_pred_mod6 :\\n', y_train_pred_mod6)\n",
    "        print('y_train :\\n', y_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred_mod6, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "        all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod6/outliers based-es_means_init: \", m )\n",
    "        \n",
    "        gmm_cluster_mod6 = GaussianMixtureMod6(n_components=n_classes, init_params='jaha_init_hybrid', means_init=None, covariance_type=cov_type, max_iter=20, random_state=0 )\n",
    "        t.tic() # Start timer\n",
    "        gmm_cluster_mod6.fit(X_train)\n",
    "        elapsed_time = t.tocvalue() #Save elapsed time\n",
    "        iter_num = gmm_cluster_mod6.n_iter_     \n",
    "        resp_labels = \"outliers based-init_hybrid_\"\n",
    "        y_train_pred_mod6 = gmm_cluster_mod6.predict(X_train)\n",
    "\n",
    "        print('y_train_pred_mod6 :\\n', y_train_pred_mod6)\n",
    "        print('y_train :\\n', y_train)\n",
    "        eval_scores = cluster_eval(y_train, y_train_pred_mod6, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "        all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod6/outliers based-init_hybrid: \", m )\n",
    "\n",
    "\n",
    "    #     #Run the GMM-OR for every th_resp value on current dataset\n",
    "    #     for resp_type in resp_type_val:\n",
    "    #         for th_resp in th_resp_val:\n",
    "    #             gmm_cluster_mod = GaussianMixtureMod(n_components=n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, th_resp=th_resp, resp_type=resp_type )\n",
    "\n",
    "    #             t.tic() # Start timer\n",
    "    #             gmm_cluster_mod.fit(X_train)\n",
    "    #             elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    #             iter_num = gmm_cluster_mod.n_iter_       \n",
    "    #             y_train_pred_mod = gmm_cluster_mod.predict(X_train)\n",
    "    #             th_resp_labels = \"GMM-\"+str(resp_type)+\"_th_resp=\"+str(th_resp)\n",
    "    #     #         print(th_resp_labels,\" time : \", elapsed_time )   \n",
    "    #             eval_scores = cluster_eval(y_train, y_train_pred_mod, X_train, th_resp_labels,m, elapsed_time, iter_num)\n",
    "    #             all_scores += [eval_scores]\n",
    "\n",
    "        #Change result to Pandas DataFrame\n",
    "        result = pd.DataFrame(all_scores)\n",
    "        all_result = all_result.append(result)\n",
    "        print(\"Finish GaussianMixtureMod : \", m)\n",
    "    all_result = all_result.append(pd.Series(), ignore_index=True) \n",
    "    all_result.columns=result_labels    \n",
    "\n",
    "\n",
    "    #Save results to Excel\n",
    "    ts = time.time() \n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%H-%M-%S---%d-%m-%Y')\n",
    "    # all_result.to_excel(\"result/All_result_\"+st+\".xlsx\", index=False)\n",
    "    all_result.to_excel(\"result/apr/All_result_\"+cov_type+\"_\"+st+\".xlsx\", index=False)\n",
    "    absolutePath += [Path(\"result/apr/All_result_\"+cov_type+\"_\"+st+\".xlsx\").resolve()]\n",
    "print(\"\\nFinish at \",st)\n",
    "\n",
    "\n",
    "\n",
    "for k in range (len(cov_type_labels)):\n",
    "    filePath = absolutePath[k]\n",
    "    os.system(f'start excel.exe \"{filePath}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main program end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(10,6))\n",
    "df1 = pd.DataFrame()\n",
    "# Make a few areas have NaN values\n",
    "df.iloc[1:3,1] = np.nan\n",
    "df.iloc[5,3] = np.nan\n",
    "df.iloc[7:9,5] = np.nan\n",
    "\n",
    "if (df.isna().any(axis=None)):\n",
    "    print(\"Nan\")\n",
    "else:\n",
    "    print(\"ok\")\n",
    "    \n",
    "if (df1.empty):\n",
    "    print(\"Nan\")\n",
    "else:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_test = y_train.copy()\n",
    "# y_train_pred_mod5_flip = np.flip(y_train_pred_mod5)\n",
    "# print(y_train_pred_mod5)\n",
    "y_train_pred_mod5_df = pd.DataFrame(y_train_pred_mod5)\n",
    "# print(y_train_pred_mod5_df) \n",
    "# print(y_train)\n",
    "\n",
    "frames = [y_train_pred_mod5_df, y_train_test ]\n",
    "y_pred_train = pd.concat(frames, axis=1)\n",
    "# print(y_pred_train)\n",
    "\n",
    "#Save results to Excel\n",
    "# ts = time.time() \n",
    "# st = datetime.datetime.fromtimestamp(ts).strftime('%H-%M-%S--%d-%m-%Y')\n",
    "y_pred_train.to_excel(\"result/apr/All_result_y_pred_train_\"+m+\"_\"+st+\".xlsx\", index=False)\n",
    "# print(y_pred_train)\n",
    "\n",
    "# filePath = [Path(\"result/apr/All_result_y_pred_train_\"+st+\".xlsx\", index=False).resolve()]\n",
    "# os.system(f'start excel.exe \"{filePath}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = [1, 1, 2, 3, 2, 3, 3]\n",
    "y_pred = [1, 1, 3, 2, 2, 2, 2]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) *100\n",
    "print('Accuracy = ', accuracy)\n",
    "\n",
    "purity = purity_score(y_true, y_pred) *100\n",
    "print('Purity   = ', purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For breastCancer\n",
    "y_train_pred_mod5 = np.choose(y_train_pred_mod5,(1,0)).astype(np.int64) \n",
    "\n",
    "#For heartDisease\n",
    "y_train_pred_mod5 = np.choose(y_train_pred_mod5,(1,0)).astype(np.int64) \n",
    "y_train_pred_mod5 = np.flip(y_train_pred_mod5)\n",
    "\n",
    "#For dermatology\n",
    "y_train_pred_mod5 = np.choose(y_train_pred_mod5,(4,5,0,1,2,3)).astype(np.int64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([3,3,3,3,3,1,1,1,1,1,0,0,0,0,0,2,2,2,2,2])\n",
    "print(a)\n",
    "print('np.choose(a,(3,1,0,2))')\n",
    "a_x = np.choose(a,(3,1,0,2)).astype(np.int64) \n",
    "print(a_x)\n",
    "print(\"\\n\")\n",
    "b= np.array([3,2,3,3,2,3,3,1,1,1,1,1,0,0,2,2,2,2,2])\n",
    "print(b)\n",
    "b_x = np.choose(b,(3,2,1,0)).astype(np.int64) \n",
    "print('np.choose(b,(3,2,1,0))')\n",
    "print(b_x)\n",
    "p\n",
    "# b= np.array([1,1,1,1,0,1,0,0,1,0,0,0])\n",
    "# print(b)\n",
    "# b_x = np.choose(b,(0,1)).astype(np.int64) \n",
    "# print('np.choose(b,(1,0))')\n",
    "# print(b_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_pred_mod5)\n",
    "y_train_pred_mod5v2 = np.choose(y_train_pred_mod5,(4,5,0,1,2,3)).astype(np.int64) \n",
    "print(y_train_pred_mod5v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_pred_mod4)\n",
    "y_train_pred_mod4v2 = np.choose(y_train_pred_mod4,(4,5,0,3,2,1)).astype(np.int64) \n",
    "print(y_train_pred_mod4v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_init in means_init : \n",
    " <class 'numpy.ndarray'> \n",
    " [[5.00555556 3.34444444 1.5962963  0.3037037 ]\n",
    " [5.97333333 2.775      4.50333333 1.46833333]\n",
    " [6.88333333 3.09722222 5.75833333 2.09444444]]\n",
    "medians_init in means_init : \n",
    " <class 'numpy.ndarray'> \n",
    " [[5.   3.4  1.5  0.2 ]\n",
    " [6.   2.8  4.5  1.4 ]\n",
    " [6.75 3.05 5.7  2.1 ]]\n",
    "    \n",
    "    means_init_hybrid : \n",
    " <class 'numpy.ndarray'> \n",
    " [[5.006      3.428      1.462      0.246     ]\n",
    " [5.95172414 2.77241379 4.35862069 1.39482759]\n",
    " [6.69047619 3.00952381 5.66190476 2.06428571]]\n",
    "medians_init_hybrid : \n",
    " <class 'numpy.ndarray'> \n",
    " [[5.   3.4  1.5  0.2 ]\n",
    " [6.   2.8  4.45 1.4 ]\n",
    " [6.7  3.   5.6  2.1 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " es_means_init2_method = np.array ([[5.006, 3.428, 1.462, 0.246],\n",
    "                                    [5.95178571, 2.77678571, 4.3375, 1.375],\n",
    "                                    [6.65681818, 2.99318182, 5.62954545, 2.05909091]])\n",
    "    \n",
    "means__init = np.array ([ [5.00555556, 3.34444444, 1.5962963,  0.3037037 ],\n",
    "                          [5.97333333, 2.775,      4.50333333, 1.46833333],\n",
    "                          [6.88333333, 3.09722222, 5.75833333, 2.09444444] ])   \n",
    "\n",
    "medians_init = np.array ([[5.,   3.4,  1.5,  0.2 ],\n",
    "                         [6.,   2.8,  4.5,  1.4 ],\n",
    "                         [6.75, 3.05, 5.7,  2.1 ]])  \n",
    "\n",
    "means__init_hybrid = np.array ([ [5.006,      3.428,      1.462,      0.246     ],\n",
    "                          [5.95172414, 2.77241379, 4.35862069, 1.39482759],\n",
    "                          [6.69047619, 3.00952381, 5.66190476, 2.06428571] ])   \n",
    "\n",
    "medians_init_hybrid = np.array ([[5.,   3.4,  1.5,  0.2 ],\n",
    "                                 [6.,   2.8,  4.45, 1.4 ],\n",
    "                                 [6.7,  3.,   5.6,  2.1 ]])  \n",
    "\n",
    "print('\\n es_means_init2_method :\\n', es_means_init2_method)\n",
    "print('\\n means__init_hybrid :\\n', means__init_hybrid)\n",
    "print('\\n medians_init_hybrid :\\n', medians_init_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions from Local File \"myfunc\"\n",
    "from ipynb.fs.full.jahaUtils import JahaInit\n",
    "\n",
    "#function to calculate means \n",
    "def means_init(X, n_components):\n",
    "    n_samples, n_attributes = X.shape\n",
    "    print(\"n_components : \", n_components)\n",
    "    \n",
    "    df_X = pd.DataFrame(X)\n",
    "    \n",
    "    df_X['mean'] = df_X.mean(axis=1)\n",
    "    \n",
    "    no_outliers_z = z_score(df_X)\n",
    "    \n",
    "    resp = np.zeros((n_samples, n_components))\n",
    "    print(resp)\n",
    "    resp = pd.DataFrame(resp)\n",
    "    print(resp)\n",
    "\n",
    "    #Find Max, Min, Diff, Dev\n",
    "    Max = max(no_outliers_z)\n",
    "    Min = min(no_outliers_z)\n",
    "    \n",
    "    diff = Max - Min\n",
    "    dev = diff/n_components\n",
    "    \n",
    "    print('Max: ', Max , 'Min: ', Min)   \n",
    "    print('dev :', dev)\n",
    "    \n",
    "    cluster_range = np.zeros((n_components, 2))\n",
    "    for i in range (n_components):\n",
    "        cluster_range[i] = Min, Min+dev\n",
    "        \n",
    "        Min = Min+dev\n",
    "        df_X['c'+str(i)]=0\n",
    "        \n",
    "        \n",
    "    print(cluster_range)  \n",
    "      \n",
    "    for j in range (n_components):\n",
    "        print(resp[j])\n",
    "        resp[j][(df_X['mean']>= cluster_range[j,0]) & (df_X['mean'] < cluster_range[j,1])] = 1\n",
    "        df_X['c'+str(j)][(df_X['mean']>= cluster_range[j,0]) & (df_X['mean'] < cluster_range[j,1])] = 1\n",
    "        \n",
    "    resp.to_excel(\"result/apr/resp.xlsx\", index=False)\n",
    "    df_X.to_excel(\"result/apr/df_X.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "    return es_init_means\n",
    "\n",
    "dataset = pd.read_csv(\"dataset/iris.csv\")\n",
    "\n",
    "#Drop Target Column in data using Index\n",
    "X = dataset.drop('Target',axis=1)\n",
    "\n",
    "# #How to get Target data\n",
    "y_train =  dataset['Target']\n",
    "\n",
    "n_components = len(np.unique(y_train))\n",
    "n_samples, _ = X.shape\n",
    "\n",
    "resp = means_init(X, n_components)\n",
    "# # if self.init_params == 'kmeans':\n",
    "# resp = np.zeros((n_samples, n_components))\n",
    "# # print(resp)\n",
    "# label = KMeans(n_clusters=n_components, n_init=1, random_state=0).fit(X).labels_\n",
    "# print(label)\n",
    "# resp[np.arange(n_samples), label] = 1\n",
    "# print(resp)\n",
    "\n",
    "# resp = np.random.rand(n_samples, n_components)\n",
    "# print(resp)\n",
    "# print(resp.sum(axis=1)[:, np.newaxis])\n",
    "# resp /= resp.sum(axis=1)[:, np.newaxis]\n",
    "# print(resp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array ([[5.005555556, 3.344444444, 1.596296296, 0.303703704],\n",
    "                        [5.973333333, 2.775, 4.503333333, 1.468333333],\n",
    "                        [6.86, 3.077142857, 5.731428571, 2.091428571]]) \n",
    "\n",
    "\n",
    "print(means)\n",
    "\n",
    "es_means_init :\n",
    " [[5.006 3.428 1.462 0.246]\n",
    " [5.936 2.77  4.26  1.326]\n",
    " [6.588 2.974 5.552 2.026]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test Mahalanobis only 11.03.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Display full output\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tqdm.auto import tqdm, trange\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 40em; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import time\n",
    "from pytictoc import TicToc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from fcmeans import FCM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture_mod import GaussianMixtureMod\n",
    "from sklearn.mixture_mod2 import GaussianMixtureMod2\n",
    "from sklearn.mixture_mod3 import GaussianMixtureMod3\n",
    "from sklearn.mixture_mod4 import GaussianMixtureMod4\n",
    "from sklearn.mixture_mod5 import GaussianMixtureMod5\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure,fowlkes_mallows_score,silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score, v_measure_score\n",
    "\n",
    "#Funtion declaration\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "\n",
    "    # Find optimal one-to-one mapping between cluster labels and true labels\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "    print(\"row_ind : \", row_ind)\n",
    "    print(\"col_ind : \", col_ind)\n",
    "    val1 = contingency_matrix[row_ind, col_ind].sum()\n",
    "    print(\"contingency_matrix[row_ind, col_ind].sum() : \", val1)\n",
    "    val2 = np.sum(contingency_matrix)\n",
    "    print(\"np.sum(contingency_matrix) : \", val2)\n",
    "\n",
    "    # Return cluster accuracy\n",
    "    return contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "    \n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "\n",
    "#Get the Clustering Algorithm Performance\n",
    "def cluster_eval(y_true, y_pred, X_train, cluster_label, m, elapsed_time,iter_num):\n",
    "    eval_scores = []\n",
    "    \n",
    "    accuracy = np.mean(y_pred.ravel() == y_true.ravel()) * 100\n",
    "    purity = purity_score(y_train, y_pred)\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1_score = f1_score(y_true, y_pred, average='weighted')\n",
    "    ari_score = adjusted_rand_score(y_true, y_pred)\n",
    "    nmi_score = normalized_mutual_info_score(y_true, y_pred)\n",
    "#     hc_v_measure = homogeneity_completeness_v_measure(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    fm_score = fowlkes_mallows_score(y_true, y_pred)\n",
    "#     s_score = silhouette_score(X_train, y_pred, metric='euclidean')\n",
    "#     db_score = davies_bouldin_score(X_train, y_pred)\n",
    "    eval_scores = [m, cluster_label, elapsed_time, iter_num, accuracy, purity, micro_f1_score, macro_f1_score, weighted_f1_score, ari_score, \n",
    "                   nmi_score, v_measure, fm_score]\n",
    "    return eval_scores\n",
    "    \n",
    "\"\"\"------------------------------------------------------------------------------------------------------------------------\n",
    "#Main Program - Start here!\n",
    "------------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# pd.set_option('display.max_rows', dataset.shape[0]+1)\n",
    "\n",
    "#Define dataset_labels\n",
    "dataset_labels = [\"iris\"]\n",
    "# dataset_labels = [\"heartDisease\",  \"transfusion\", \"wilt\", \"balanceScale\",\"dermatology\"]  \n",
    "# dataset_labels = [\"heartDisease\", \"transfusion\", \"wilt\", \"breastCancer\", \"chess\", \n",
    "#                   \"parkinson\",\"australian\", \"german\", \"japanese\", \"iris\", \"abalone\", \"wallRobot\",\n",
    "#                   \"dermatology\",\"creditApproval\", \"letters\", \"eegEyeState\",\n",
    "#                   \"seismicBumps\",\"bank\", \"adultIncome\", \"sensorlessDrive\"]\n",
    "#Final dataset tested on 11.03.2021\n",
    "# dataset_labels = [\"transfusion\", \"wilt\", \"breastCancer\", \"heartDisease\", \"eegEyeState\", \"adultIncome\",\n",
    "#                   \"australian\", \"japanese\", \"bank\", \"seismicBumps\", \"german\", \"chess\", \n",
    "#                   \"iris\", \"abalone\", \"wallRobot\", \"dermatology\"]\n",
    "# dataset_labels = [\"seismicBumps\",\"bank\", \"adultIncome\"]\n",
    "# skip_Mahal = [\"seismicBumps\",\"bank\", \"adultIncome\"]\n",
    "skip_Mahal = [\"seismicBumps\",\"bank\"]\n",
    "# Threshold of GMM-EM Response values\n",
    "th_resp_val = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# th_resp_val = [1]\n",
    "\n",
    "# resp_type_val = ['donate']\n",
    "# resp_type_val = ['donate']\n",
    "resp_type_val = ['remove', 'donate']\n",
    "\n",
    "result_labels = [\"Dataset\",\"Algorithm\", \"Elapsed Time\", \"Iter_num\",\"accuracy\", \"purity\", \"micro_f1_score\", \"macro_f1_score\", \"weighted_f1_score\", \"ari_score\", \n",
    "                \"nmi_score\", \"hc_v_measure\", \"fm_score\"]\n",
    "\n",
    "all_scores = []\n",
    "final_scores = []\n",
    "all_result = pd.DataFrame()\n",
    "t = TicToc() # create TicToc instance\n",
    "# for m in dataset_labels:\n",
    "for m in tqdm(dataset_labels):\n",
    "#     enablePrint()\n",
    "    print(\"\\n\\nStart Dataset : \",m)\n",
    "    #Read dataset\n",
    "    dataset = pd.read_csv(\"dataset/\"+m+\".csv\")\n",
    "\n",
    "    #Drop Target Column in data using Index\n",
    "    X_train = dataset.drop('Target',axis=1)\n",
    "\n",
    "    # #How to get Target data\n",
    "    y_train =  dataset['Target']\n",
    "\n",
    "    print(np.array(y_train))\n",
    "\n",
    "    n_classes = len(np.unique(y_train))\n",
    "#     cov_type = 'full'\n",
    "    cov_type = 'tied'\n",
    "#     cov_type = 'diag'\n",
    "    #Since we have class labels for the training data, we can initialize the GMM parameters in a supervised manner.\n",
    "    es_means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)])\n",
    "#     print(\"es_means_init :\\n\",es_means_init)\n",
    "\n",
    "    #Run the kMeans on current\n",
    "    kMeans_cluster = KMeans(n_clusters = n_classes, max_iter=20, random_state=0)\n",
    "    t.tic() # Start timer\n",
    "    kMeans_cluster.fit(X_train)\n",
    "    elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    iter_num = kMeans_cluster.n_iter_\n",
    "#     print(\"kMeans time : \", elapsed_time )   \n",
    "    y_train_pred = kMeans_cluster.predict(X_train)\n",
    "    eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"kMeans\", m, elapsed_time, iter_num )\n",
    "#     eval_scores = np.append([m],eval_scores,axis=0)  \n",
    "    all_scores = [eval_scores]\n",
    "    \n",
    "#     #Run the Meanshift on current\n",
    "#     ms_cluster = MeanShift(bandwidth=n_classes)\n",
    "#     t.tic() # Start timer\n",
    "#     ms_cluster = ms_cluster.fit(X_train)\n",
    "#     elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#     iter_num = \"NA\" #ms_cluster.n_iter_\n",
    "# #     print(\"GMM time : \", elapsed_time )       \n",
    "#     y_train_pred = ms_cluster.predict(X_train)\n",
    "#     eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"Meanshift\", m, elapsed_time, iter_num )\n",
    "# #     all_scores += [eval_scores]\n",
    "#     all_scores += [eval_scores]\n",
    "    \n",
    "#     #Run the FCM on current\n",
    "#     fcm_cluster = FCM(n_clusters=n_classes)\n",
    "#     t.tic() # Start timer\n",
    "#     fcm_cluster.fit(X_train)\n",
    "#     elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#     iter_num = \"NA\" #fcm_cluster.n_iter_\n",
    "# #     print(\"GMM time : \", elapsed_time )       \n",
    "#     y_train_pred = fcm_cluster.predict(X_train)\n",
    "#     eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"FCM\", m, elapsed_time, iter_num )\n",
    "# #     all_scores += [eval_scores]\n",
    "#     all_scores += [eval_scores]\n",
    "\n",
    "    #Run the GMM on current\n",
    "    gmm_cluster = GaussianMixture(n_components = n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "    t.tic() # Start timer\n",
    "    gmm_cluster.fit(X_train)\n",
    "    elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    iter_num = gmm_cluster.n_iter_\n",
    "#     print(\"GMM time : \", elapsed_time )       \n",
    "    y_train_pred = gmm_cluster.predict(X_train)\n",
    "    eval_scores = cluster_eval(y_train, y_train_pred, X_train, \"GMM\", m, elapsed_time, iter_num )\n",
    "#     all_scores += [eval_scores]\n",
    "    all_scores += [eval_scores]\n",
    "\n",
    "    #Run the GMM Mahalanobis on current - Last tested on 28.01.2021\n",
    "    if m not in (skip_Mahal):    \n",
    "        for resp_type in resp_type_val:\n",
    "            gmm_cluster_mod2 = GaussianMixtureMod2(n_components = n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type)\n",
    "            t.tic() # Start timer\n",
    "            gmm_cluster_mod2.fit(X_train)\n",
    "            elapsed_time = t.tocvalue() #Save elapsed time\n",
    "            iter_num = gmm_cluster_mod2.n_iter_\n",
    "        #     print(\"GMM time : \", elapsed_time ) \n",
    "            resp_labels = \"Mahalanobis-\"+str(resp_type)\n",
    "            y_train_pred_mod2 = gmm_cluster_mod2.predict(X_train)\n",
    "            eval_scores = cluster_eval(y_train, y_train_pred_mod2, X_train, resp_labels, m, elapsed_time, iter_num )\n",
    "            all_scores += [eval_scores]\n",
    "        print(\"Finish GaussianMixtureMod2/Mahalanobis : \",m,\" : \",resp_type )\n",
    "    else:\n",
    "        print(\"Skip GaussianMixtureMod2/Mahalanobis for :\",m )\n",
    " \n",
    "    # Run the GMM-Auto for every th_resp value on current dataset (using intersect)- Last tested on 27.01.2021\n",
    "    #     for resp_type in resp_type_val:\n",
    "    #         gmm_cluster_mod3 = GaussianMixtureMod3(n_components=n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "    #         t.tic() # Start timer\n",
    "    #         gmm_cluster_mod3.fit(X_train)\n",
    "    #         elapsed_time = t.tocvalue() #Save elapsed time\n",
    "    #         iter_num = gmm_cluster_mod3.n_iter_     \n",
    "    #         resp_labels = \"GMM_Auto-\"+str(resp_type)\n",
    "    #         y_train_pred_mod3 = gmm_cluster_mod3.predict(X_train)\n",
    "    #         eval_scores = cluster_eval(y_train, y_train_pred_mod3, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "    #         all_scores += [eval_scores]\n",
    "    #     print(\"Finish GaussianMixtureMod3 : \", m,\" : \",resp_type )\n",
    "\n",
    "#     #Run the GMM-Auto-v2 for every th_resp value on current dataset (using weights*cv or cv/2) - Last tested on 27.01.2021\n",
    "#     for resp_type in resp_type_val:\n",
    "#         gmm_cluster_mod4 = GaussianMixtureMod4(n_components=n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "#         t.tic() # Start timer\n",
    "#         gmm_cluster_mod4.fit(X_train)\n",
    "#         elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#         iter_num = gmm_cluster_mod4.n_iter_     \n",
    "#         resp_labels = \"resp Mixing Coefficients -\"+str(resp_type)\n",
    "#         y_train_pred_mod4 = gmm_cluster_mod4.predict(X_train)\n",
    "#         eval_scores = cluster_eval(y_train, y_train_pred_mod4, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "#         all_scores += [eval_scores]\n",
    "#     print(\"Finish GaussianMixtureMod4/resp Mixing Coefficients  : \", m, \" : \",resp_type )\n",
    "\n",
    "#     #Run the GMM-Auto-v2 for every th_resp value on current dataset (using resp center values) - Last tested on 27.01.2021\n",
    "#     for resp_type in resp_type_val:\n",
    "#         gmm_cluster_mod5 = GaussianMixtureMod5(n_components=n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, resp_type=resp_type )\n",
    "#         t.tic() # Start timer\n",
    "#         gmm_cluster_mod5.fit(X_train)\n",
    "#         elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#         iter_num = gmm_cluster_mod5.n_iter_     \n",
    "#         resp_labels = \"resp center values-\"+str(resp_type)\n",
    "#         y_train_pred_mod5 = gmm_cluster_mod5.predict(X_train)\n",
    "#         eval_scores = cluster_eval(y_train, y_train_pred_mod5, X_train, resp_labels,m, elapsed_time, iter_num)\n",
    "#         all_scores += [eval_scores]\n",
    "#     print(\"Finish GaussianMixtureMod5/resp center values : \", m, \" : \",resp_type )\n",
    "\n",
    "#     #Run the GMM-OR for every th_resp value on current dataset\n",
    "#     for resp_type in resp_type_val:\n",
    "#         for th_resp in th_resp_val:\n",
    "#             gmm_cluster_mod = GaussianMixtureMod(n_components=n_classes, init_params='random', means_init=es_means_init, covariance_type=cov_type, max_iter=20, random_state=0, th_resp=th_resp, resp_type=resp_type )\n",
    "\n",
    "#             t.tic() # Start timer\n",
    "#             gmm_cluster_mod.fit(X_train)\n",
    "#             elapsed_time = t.tocvalue() #Save elapsed time\n",
    "#             iter_num = gmm_cluster_mod.n_iter_       \n",
    "#             y_train_pred_mod = gmm_cluster_mod.predict(X_train)\n",
    "#             th_resp_labels = \"GMM-\"+str(resp_type)+\"_th_resp=\"+str(th_resp)\n",
    "#     #         print(th_resp_labels,\" time : \", elapsed_time )   \n",
    "#             eval_scores = cluster_eval(y_train, y_train_pred_mod, X_train, th_resp_labels,m, elapsed_time, iter_num)\n",
    "#             all_scores += [eval_scores]\n",
    "\n",
    "    #Change result to Pandas DataFrame\n",
    "    result = pd.DataFrame(all_scores)\n",
    "    all_result = all_result.append(result)\n",
    "#     print(\"Finish GaussianMixtureMod : \", m, \" : \",resp_type )\n",
    "all_result = all_result.append(pd.Series(), ignore_index=True) \n",
    "all_result.columns=result_labels    \n",
    "\n",
    "\n",
    "#Save results to Excel\n",
    "ts = time.time() \n",
    "st = datetime.datetime.fromtimestamp(ts).strftime('%H-%M-%S---%d-%m-%Y')\n",
    "# all_result.to_excel(\"result/All_result_\"+st+\".xlsx\", index=False)\n",
    "all_result.to_excel(\"result/All_result_\"+cov_type+\"_\"+st+\".xlsx\", index=False)\n",
    "print(\"\\nFinish at \",st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Display full output\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tqdm.auto import tqdm, trange\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 40em; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import time\n",
    "from pytictoc import TicToc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from fcmeans import FCM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture_mod import GaussianMixtureMod\n",
    "from sklearn.mixture_mod2 import GaussianMixtureMod2\n",
    "from sklearn.mixture_mod3 import GaussianMixtureMod3\n",
    "from sklearn.mixture_mod4 import GaussianMixtureMod4\n",
    "from sklearn.mixture_mod5 import GaussianMixtureMod5\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure,fowlkes_mallows_score,silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score, v_measure_score\n",
    "\n",
    "#Funtion declaration\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "\n",
    "    # Find optimal one-to-one mapping between cluster labels and true labels\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "    print(\"row_ind : \", row_ind)\n",
    "    print(\"col_ind : \", col_ind)\n",
    "    val1 = contingency_matrix[row_ind, col_ind].sum()\n",
    "    print(\"contingency_matrix[row_ind, col_ind].sum() : \", val1)\n",
    "    val2 = np.sum(contingency_matrix)\n",
    "    print(\"np.sum(contingency_matrix) : \", val2)\n",
    "\n",
    "    # Return cluster accuracy\n",
    "    return contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "#     print(\"\\ncontingency_matrix : \\n\", contingency_matrix)\n",
    "    \n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "\n",
    "#Get the Clustering Algorithm Performance\n",
    "def cluster_eval(y_true, y_pred, X_train, cluster_label, m, elapsed_time,iter_num):\n",
    "    eval_scores = []\n",
    "    \n",
    "    accuracy = np.mean(y_pred.ravel() == y_true.ravel()) * 100\n",
    "    purity = purity_score(y_train, y_pred)\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1_score = f1_score(y_true, y_pred, average='weighted')\n",
    "    ari_score = adjusted_rand_score(y_true, y_pred)\n",
    "    nmi_score = normalized_mutual_info_score(y_true, y_pred)\n",
    "#     hc_v_measure = homogeneity_completeness_v_measure(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    fm_score = fowlkes_mallows_score(y_true, y_pred)\n",
    "#     s_score = silhouette_score(X_train, y_pred, metric='euclidean')\n",
    "#     db_score = davies_bouldin_score(X_train, y_pred)\n",
    "    eval_scores = [m, cluster_label, elapsed_time, iter_num, accuracy, purity, micro_f1_score, macro_f1_score, weighted_f1_score, ari_score, \n",
    "                   nmi_score, v_measure, fm_score]\n",
    "    return eval_scores\n",
    "    \n",
    "\"\"\"------------------------------------------------------------------------------------------------------------------------\n",
    "#Main Program - Start here!\n",
    "------------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# pd.set_option('display.max_rows', dataset.shape[0]+1)\n",
    "\n",
    "#Define dataset_labels\n",
    "dataset_labels = [\"iris\"]\n",
    "# dataset_labels = [\"heartDisease\",  \"transfusion\", \"wilt\", \"balanceScale\",\"dermatology\"]  \n",
    "# dataset_labels = [\"heartDisease\", \"transfusion\", \"wilt\", \"breastCancer\", \"chess\", \n",
    "#                   \"parkinson\",\"australian\", \"german\", \"japanese\", \"iris\", \"abalone\", \"wallRobot\",\n",
    "#                   \"dermatology\",\"creditApproval\", \"letters\", \"eegEyeState\",\n",
    "#                   \"seismicBumps\",\"bank\", \"adultIncome\", \"sensorlessDrive\"]\n",
    "#Final dataset tested on 11.03.2021\n",
    "# dataset_labels = [\"transfusion\", \"wilt\", \"breastCancer\", \"heartDisease\", \"eegEyeState\", \"adultIncome\",\n",
    "#                   \"australian\", \"japanese\", \"bank\", \"seismicBumps\", \"german\", \"chess\", \n",
    "#                   \"iris\", \"abalone\", \"wallRobot\", \"dermatology\"]\n",
    "# dataset_labels = [\"seismicBumps\",\"bank\", \"adultIncome\"]\n",
    "# skip_Mahal = [\"seismicBumps\",\"bank\", \"adultIncome\"]\n",
    "skip_Mahal = [\"seismicBumps\",\"bank\"]\n",
    "# Threshold of GMM-EM Response values\n",
    "th_resp_val = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# th_resp_val = [1]\n",
    "\n",
    "# resp_type_val = ['donate']\n",
    "# resp_type_val = ['donate']\n",
    "resp_type_val = ['remove', 'donate']\n",
    "\n",
    "result_labels = [\"Dataset\",\"Algorithm\", \"Elapsed Time\", \"Iter_num\",\"accuracy\", \"purity\", \"micro_f1_score\", \"macro_f1_score\", \"weighted_f1_score\", \"ari_score\", \n",
    "                \"nmi_score\", \"hc_v_measure\", \"fm_score\"]\n",
    "\n",
    "all_scores = []\n",
    "final_scores = []\n",
    "all_result = pd.DataFrame()\n",
    "t = TicToc() # create TicToc instance\n",
    "# for m in dataset_labels:\n",
    "for m in tqdm(dataset_labels):\n",
    "#     enablePrint()\n",
    "    print(\"\\n\\nStart Dataset : \",m)\n",
    "    #Read dataset\n",
    "    dataset = pd.read_csv(\"dataset/\"+m+\".csv\")\n",
    "\n",
    "    #Drop Target Column in data using Index\n",
    "    X_train = dataset.drop('Target',axis=1)\n",
    "\n",
    "    # #How to get Target data\n",
    "    y_train =  dataset['Target']\n",
    "    \n",
    "    print(np.array(y_train))\n",
    "\n",
    "    n_classes = len(np.unique(y_train))\n",
    "#     cov_type = 'full'\n",
    "    cov_type = 'tied'\n",
    "    \n",
    "    #Since we have class labels for the training data, we can initialize the GMM parameters in a supervised manner.\n",
    "    es_means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)])\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import time\n",
    "from pytictoc import TicToc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MeanShift    print(\"es_means_init : \\n\", es_means_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find means_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import time\n",
    "from pytictoc import TicToc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "dataset_labels = [\"iris\"]\n",
    "init_params = 'gmm'\n",
    "\n",
    "n_components = len(np.unique(y_train))\n",
    "random_state=0\n",
    "\n",
    "for m in tqdm(dataset_labels):\n",
    "    print(\"\\n\\nStart Dataset : \",m)\n",
    "    \n",
    "    #Read dataset\n",
    "    dataset = pd.read_csv(\"dataset/\"+m+\".csv\")\n",
    "\n",
    "    #Drop Target Column in data using Index\n",
    "    X_train = dataset.drop('Target',axis=1)\n",
    "\n",
    "    # #How to get Target data\n",
    "    y_train =  dataset['Target']\n",
    "\n",
    "   \n",
    "    X = X_train\n",
    "    \n",
    "    n_samples, n_attributes = X.shape\n",
    "    \n",
    "#     print(n_attributes)\n",
    "    \n",
    "    df_X = pd.DataFrame(X)\n",
    "    \n",
    "    #Find rows average\n",
    "    df_X['mean'] = df_X.mean(axis=1)\n",
    "     \n",
    "    \n",
    "    #Find Max, Min, Diff, Dev\n",
    "    Max = max(df_X['mean'])\n",
    "    Min = min(df_X['mean'])\n",
    "    \n",
    "    diff = Max - Min\n",
    "    dev = diff/n_components\n",
    "    \n",
    "    print('Max: ', Max , 'Min: ', Min)   \n",
    "    print('dev :', dev)\n",
    "    \n",
    "    cluster_range = np.zeros((n_components, 2))\n",
    "    for i in range (n_components):\n",
    "        cluster_range[i] = Min, Min+dev\n",
    "        \n",
    "        Min = Min+dev\n",
    "        df_X['c'+str(i)]=0\n",
    "#     print(cluster_range)  \n",
    "\n",
    "    for j in range (n_components):\n",
    "        df_X['c'+str(j)][(df_X['mean']>= cluster_range[j,0]) & (df_X['mean'] < cluster_range[j,1])] = 1\n",
    "    \n",
    "   \n",
    "    es_init_means = np.zeros((n_components, n_attributes))\n",
    "    for k in range (n_components):    \n",
    "        temp_df_X = df_X.iloc[:, 0:n_attributes][df_X['c'+str(k)] == 1]   \n",
    "        es_init_means[k] = temp_df_X.mean(axis=0)\n",
    "        \n",
    "#         print('es_init_means[',k,'] : ',es_init_means[k])\n",
    "    print(\"es_init_means : \\n\", es_init_means)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     if init_params == 'kmeans':\n",
    "#         resp = np.zeros((n_samples, n_components))\n",
    "#         label = KMeans(n_clusters= n_components, n_init=1,\n",
    "#                                random_state=random_state).fit(X).labels_\n",
    "#         resp[np.arange(n_samples), label] = 1\n",
    "#         print('label : ', label)\n",
    "#         print('resp  :  \\n', resp)\n",
    "    \n",
    "#     elif init_params == 'gmm':\n",
    "#         resp = np.zeros((n_samples, n_components))\n",
    "#         gmm_cluster = GaussianMixture(n_components= n_components,  init_params='random', random_state=random_state).fit(X)\n",
    "#         label = gmm_cluster.predict(X)\n",
    "#         resp[np.arange(n_samples), label] = 1\n",
    "#         print('label : ', label)\n",
    "#         print('resp  :  \\n', resp)\n",
    "        \n",
    "#     elif init_params == 'random':\n",
    "#         resp = random_state.rand(n_samples, n_components)\n",
    "#         resp /= resp.sum(axis=1)[:, np.newaxis]\n",
    "#         resp1_ = resp\n",
    "#     else:\n",
    "#         raise ValueError(\"Unimplemented initialization method '%s'\"\n",
    "#                      % self.init_params)\n",
    "\n",
    "    es_means_init = np.array ([[5.005555556, 3.344444444, 1.596296296, 0.303703704],\n",
    "                                [5.973333333, 2.775, 4.503333333, 1.468333333],\n",
    "                                [6.86, 3.077142857, 5.731428571, 2.091428571]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_X)\n",
    "df_X_new = df_X.iloc[:, 0:n_attributes]\n",
    "print(df_X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(n_components)\n",
    "Min = 2.1\n",
    "dev = 1\n",
    "Max=5.1\n",
    "cluster_range = np.zeros((n_components, 2))\n",
    "for i in range (n_components):\n",
    "    print(i, Min)\n",
    "#     cluster_range[i] = i, i+1\n",
    "    cluster_range[i] = Min, Min+dev\n",
    "#     min_range[i] = i+1\n",
    "# #     max_range[i] = Min+dev\n",
    "    Min = Min+dev\n",
    "print(cluster_range)\n",
    "#     print(Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(df_X['mean']))   \n",
    "print(min(df_X['mean'])) \n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.array([[0.10605892, 0.27159891, 0.53311513, 0.29504658, 0.43389628],\n",
    " [0.10532298, 0.30813951, 0.53196672, 0.35493489, 0.52814269]])\n",
    "\n",
    "\n",
    "o_cv = np.amax(cv, axis=1)\n",
    "# cv = np.asarray(cv, dtype=np.int32)\n",
    "print(o_cv)\n",
    "for i in range (len(o_cv)):\n",
    "    print(i,\"o_cv:\",o_cv[i])\n",
    "    cv = o_cv[i]\n",
    "    print(i,\"cv :\",cv)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = [\"heartDisease\", \"transfusion\", \"wilt\", \"breastCancer\", \"chess\", \n",
    "                  \"parkinson\",\"australian\", \"german\", \"japanese\", \"iris\", \"abalone\", \"wallRobot\"\n",
    "                  \"dermatology\",\"creditApproval\", \"letters\", \"eegEyeState\",\n",
    "                  \"seismicBumps\",\"bank\", \"adultIncome\", \"sensorlessDrive\", \"creditCard\"]\n",
    "\n",
    "skip_Mahal = [\"seismicBumps\",\"bank\", \"adultIncome\", \"sensorlessDrive\", \"creditCard\"]\n",
    "\n",
    "for m in tqdm(dataset_labels):\n",
    "    if m not in (skip_Mahal):\n",
    "        print(\"Run Mahalanobis for :\",m )\n",
    "    else:\n",
    "        print(\"Skip Mahalanobis for :\",m )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_result)\n",
    "all_result = all_result.append(pd.Series(), ignore_index=True) \n",
    "all_result.columns=result_labels \n",
    "\n",
    "#Save results to Excel\n",
    "ts = time.time() \n",
    "st = datetime.datetime.fromtimestamp(ts).strftime('%H-%M-%S---%d-%m-%Y')\n",
    "print(st)\n",
    "all_result.to_excel(\"result/All_result_\"+cov_type+\"_\"+st+\".xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
